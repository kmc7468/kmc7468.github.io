---
layout: post
title: 역전파
subtitle: 연쇄법칙을 효율적으로 계산하는 한 방법
category: ml
tag: [머신러닝, 신경망, 역전파, 수학, 미분, 미적분]
comment: true
---

안녕하세요, static입니다. 2019년의 첫번째 글입니다. 2019년의 첫번째 글을 머신러닝과 관련된 글로 작성한다는 것이 굉장히 기분이 좋네요. 이번 글에서는 인공신경망에서 굉장히 중요한 알고리즘 중 하나인 역전파의 수학적인 부분에 대해서 알아보겠습니다.

## 미분

역전파에 대해 알기 전, 역전파가 미분 알고리즘이기 때문에 미분에 대해 어느정도 알고 갈 필요가 있습니다. 너무 겁먹진 마세요! 간단한 내용만 알고 갈겁니다.

$$ f(x)=nx^a \\ f'(x)=anx^{a-1} $$

$$f'(x)$$는 $$f(x)$$를 미분한 함수로, **도함수**라고 부릅니다. $$a$$가 *상수일 때*에는 위 미분 공식이 성립합니다. 이를 통해서 역전파를 공부할 때 유용하게 쓰일 공식 한가지를 유도할 수 있습니다.

$$ f(x)=ax \\ f'(x)=a $$

이는 위의 수식에서 $$a=1$$일 경우의 식입니다. 식을 한국어로 설명 드리자면, 1차항이 있을 때 그 항의 기울기는 계수라는 뜻입니다. 당연한 내용이라 설명할 필요도 없는 말이죠. 중학교 수학을 배우신 분이라면 아실 내용입니다.

$$ f(x)=f_1(x)+f_2(x)+...+f_n(x) \\ f'(x)=f'_1(x)+f'_2(x)+...+f'_n(x) $$

위 식을 설명드리면, 어떤 함수 $$f(x)$$가 여러 함수들의 합으로 이루어진 함수일 경우, $$f'(x)$$는 구성하는 함수들의 도함수들의 합과 같다는 뜻입니다.

$$ f(x)=C \\ f'(x)=0 $$

$$C$$가 상수일 때, 미분하면 상수는 사라진다는 의미를 갖습니다. 이정도 지식이 있으시면 역전파를 공부하는데 큰 어려움은 없으실겁니다. 그럼 역전파를 향해서 발걸음을 나아가 봅시다.

### 미분의 연쇄법칙

$$ y=g(x), z=f(y) \\ \frac{dz}{dx}=\frac{dz}{dy}\frac{dy}{dx}=f'(y)g'(x) $$

**미분의 연쇄법칙**은 합성함수의 미분 공식이라고 볼 수 있는데, 예를 들어 함수 $$f$$와 $$g$$의 합성으로 만들어진 함수는 두 함수의 도함수를 곱하면 됩니다. 여러개의 합성으로 만들어진 함수도 마찬가지로, 계속 곱해나가면 됩니다. 이것이 바로 역전파의 초석이 됩니다.

## 역전파

역전파는 인공신경망 같은 곳에서 많이 쓰이는 미분 알고리즘으로, 미분을 계산할 때 필요한 비용을 크게 줄여 인공신경망을 발전하게 한 큰 주역입니다. 역전파는 한번 계산한 미분을 저장해 두고 다른 미분을 계산할 때 그 값을 계산해야 할 필요가 있을 때 그 값을 쓰도록 함으로써 인공신경망의 첫번째 층에 가까워질 수록 기하급수적으로 증가하는 계산 비용을 크게 줄였습니다. 역전파는 미분의 연쇄법칙을 이용한 알고리즘인데, 역전파라는 이름이 붙은 이유는 미분을 계산하는 방향이 순전파와는 반대 방향이기 때문에 역전파라는 이름이 붙었습니다.

### 2층 신경망의 예

우선, 은닉층과 출력층 각각 1개씩 있는 간단한 2층 신경망의 예로 역전파에 대해 설명 드리겠습니다.

$$\mathbf{x}$$[^1]가 입력이며, $$\mathbf{y}$$가 출력입니다. $$f$$, $$g$$가 각각 은닉층, 출력층의 활성화 함수입니다. $$\mathbf{W}^{(1)}$$, $$\mathbf{W}^{(2)}$$[^2]는 각각 은닉층, 출력층의 가중치이며, $$\mathbf{b}^{(1)}$$, $$\mathbf{b}^{(2)}$$는 각각 은닉층, 출력층의 바이아스(bias, 편향) 값입니다. $$\mathbf{J}$$를 손실함수(오차함수)의 손실값이라 할 때, 우리는 학습을 위해 $$\frac{\partial \mathbf{J}}{\partial \mathbf{W}^{(1)}}$$, $$\frac{\partial \mathbf{J}}{\partial \mathbf{W}^{(2)}}$$, $$\frac{\partial \mathbf{J}}{\partial \mathbf{b}^{(1)}}$$, $$\frac{\partial \mathbf{J}}{\partial \mathbf{b}^{(2)}}$$를 계산해야 합니다. 그래야 이 미분 값들을 바탕으로 경사하강법을 실행할 수 있기 때문입니다.

역전파는 출력층부터 시작해서 차근차근 미분을 계산해 나가고, 첫번째 층의 미분을 계산하면 종료하게 됩니다. 우선, $$\mathbf{s}^{(1)}=\mathbf{W}^{(1)}\mathbf{x}+\mathbf{b}^{(1)}, \mathbf{h}^{(1)}=f(\mathbf{s}^{(1)})$$, $$\mathbf{s}^{(2)}=\mathbf{W}^{(2)}\mathbf{s}^{(1)}+\mathbf{b}^{(2)}, \mathbf{h}^{(2)}=f(\mathbf{s}^{(2)})=\mathbf{y}$$라고 하겠습니다. 즉, 층의 출력은 $$\mathbf{h}$$이며 층의 가중합은 $$\mathbf{s}$$입니다. 우선 출력층의 가중치와 바이아스 값에 대한 손실값의 기울기부터 구해보겠습니다. 우선 가중치부터 구해볼게요. 손실함수를 $$L$$이라 하면, $$\mathbf{J}=L(g(\mathbf{W}^{(2)}\mathbf{h}^{(1)}+\mathbf{b}^{(2)}))$$입니다. 그렇기 때문에, $$\frac{\partial \mathbf{J}}{\partial \mathbf{W}^{(2)}}$$는 연쇄법칙을 적용해서 $$\frac{\partial \mathbf{J}}{\partial \mathbf{W}^{(2)}}=\frac{\partial \mathbf{J}}{\partial g}\frac{\partial g}{\partial \mathbf{s}^{(2)}}\frac{\partial \mathbf{s}^{(2)}}{\partial \mathbf{W}^{(2)}}$$가 됩니다. 우선, 저희는 $$L$$과 $$g$$가 무슨 함수인지 모르기 때문에, 오른쪽 항의 첫번째와 두번째 미분을 그냥 $$L'$$와 $$g'$$로 두겠습니다. 그러면 우리는 $$\frac{\partial \mathbf{s}^{(2)}}{\partial \mathbf{W}^{(2)}}$$만 구하면 되겠네요. 위에서 설명했듯이, 1차항의 기울기는 변수에 대한 계수라고 했습니다. $$\partial$$은 **편미분** 기호인데요, 변수가 여러개더라도 하나만 변수로 두고 나머지는 모두 상수로 보고 미분하겠다는 뜻입니다. $$\mathbf{s}^{(2)}$$를 계산할 때 $$\mathbf{W}^{(2)}$$에 곱한 수는, 즉 계수는 $$\mathbf{h}^{(1)}$$이 되겠네요. 따라서 $$\frac{\partial \mathbf{s}^{(2)}}{\partial \mathbf{W}^{(2)}}=\mathbf{h}^{(1)}$$입니다. $$\mathbf{b}^{(2)}$$가 어디로 사라졌는지 궁금해 하실 분도 있을 것입니다. 위에서 설명했듯이 상수는 어짜피 미분하면 사라지므로 사라지는게 정상입니다. 정리하면, $$\frac{\partial \mathbf{J}}{\partial \mathbf{W}^{(2)}}=L'(\mathbf{h}^{(2)})g'(\mathbf{s}^{(2)})\mathbf{h}^{(1)}$$입니다. 이렇게 해서 출력층의 가중치에 대한 손실값의 기울기는 구했습니다. 그럼 바이아스 값에 대한 기울기는 어떻게 구할까요? $$\frac{\partial \mathbf{J}}{\partial \mathbf{b}^{(2)}}=L'(\mathbf{h}^{(2)})g'(\mathbf{s}^{(2)})$$입니다. 한번 이렇게 생각해 보세요. 바이아스 값을 담는 변수를 별도로 두지 말고, 그냥 가중치를 담는 변수에 포함시켰다고 생각해 봅시다. $$\mathbf{w}=\{b, w_1, w_2, ...\}$$가 되는 것이죠. 대신, 층의 입력의 가장 첫번째에 1이라는 요소를 넣습니다. $$\mathbf{x}=\{1, x_1, x_2, ...\}$$. 그리고 두 벡터의 내적을 계산해보면, 자연스럽게 $$\mathbf{wx}+b$$와 동치의 값이 계산되는걸 확인할 수 있습니다. 이러한 맥락에서, $$\mathbf{b}^{(2)}$$에 곱해진 수, 즉 기울기를 1로 보는 것입니다.

같은 방법으로 첫번째 층의 가중치와 바이아스 값에 대한 손실값의 기울기도 계산할 수 있습니다. 가중치에 대한 기울기부터 계산해 보겠습니다. 연쇄법칙을 적용하면 $$\frac{\partial \mathbf{J}}{\partial \mathbf{W}^{(1)}}=\frac{\partial \mathbf{J}}{\partial g}\frac{\partial g}{\partial \mathbf{s}^{(2)}}\frac{\partial \mathbf{s}^{(2)}}{\partial f}\frac{\partial f}{\partial \mathbf{s}^{(1)}}\frac{\partial \mathbf{s}^{(1)}}{\partial \mathbf{W}^{(1)}}$$이 되는데, 오른쪽 항의 처음 두번째 기울기는 이미 위에서 계산했고, $$\frac{\partial f}{\partial \mathbf{s}^{(1)}}$$는 $$f$$가 어떤 함수인지 모르므로 마찬가지로 $$f'$$로 두면 $$\frac{\partial \mathbf{s}^{(2)}}{\partial f}$$와 $$\frac{\partial \mathbf{s}^{(1)}}{\partial \mathbf{W}^{(1)}}$$만 구하면 됩니다. $$\frac{\partial \mathbf{s}^{(2)}}{\partial f}=\mathbf{W}^{(2)}$$인데, $$\mathbf{h}^{(1)}=f(\mathbf{s}^{(1)})$$이며, $$\mathbf{s}^{(2)}=\mathbf{W}^{(2)}\mathbf{h}^{(1)}+\mathbf{b}^{(2)}$$이기 때문입니다. 즉, $$\mathbf{s}^{(2)}$$를 계산할 때, $$\mathbf{h}^{(1)}$$에 곱해진 수, 즉 기울기가 $$\mathbf{W}^{(2)}$$이기 때문입니다. $$\frac{\partial \mathbf{s}^{(1)}}{\partial \mathbf{W}^{(1)}}=\mathbf{x}$$인데, 이유는 앞과 동일합니다. $$\mathbf{s}^{(1)}=\mathbf{W}^{(1)}\mathbf{x}+\mathbf{b}^{(1)}$$인데, $$\mathbf{W}^{(1)}$$에 곱해진 수, 즉 기울기가 $$\mathbf{x}$$이기 때문입니다. 정리하면, $$\frac{\partial \mathbf{J}}{\partial \mathbf{W}^{(1)}}=L'(\mathbf{h}^{(2)})g'(\mathbf{s}^{(2)})\mathbf{W}^{(2)}f'(\mathbf{s}^{(1)})\mathbf{x}$$입니다. 바이아스 값의 기울기는 $$\frac{\partial \mathbf{J}}{\partial \mathbf{W}^{(1)}}=L'(\mathbf{h}^{(2)})g'(\mathbf{s}^{(2)})\mathbf{W}^{(2)}f'(\mathbf{s}^{(1)})$$입니다. 이유는 출력층의 바이아스 값의 기울기를 계산할 때와 같으므로 생략하겠습니다.

### 다층 신경망으로의 일반화

그럼 2층 신경망을 통해 역전파 알고리즘에 대해 어느정도 파악했으니, 다층 신경망에 대해 일반화를 해보겠습니다. 우선, 신경망의 깊이를 $$D$$이라고 하고, 자연수 $$d\in\{\alpha\mid1\le\alpha\le D\}$$를 정의하겠습니다. $$d$$번째 층의 활성화 함수를 $$f^{(d)}$$, $$\mathbf{h}^{(0)}=\mathbf{x}$$라고 하면, 다음과 같이 일반화 할 수 있습니다.

$$ \frac{\partial \mathbf{J}}{\partial \mathbf{b}^{(d)}}=\mathbf{g}^{(d+1)}f'^{(d)}(\mathbf{s}^{(d)}) \\ \frac{\partial \mathbf{J}}{\partial \mathbf{W}^{(d)}}=\frac{\partial \mathbf{J}}{\partial \mathbf{b}^{(d)}}\mathbf{h}^{(d-1)} \\ \mathbf{g}^{(d)}=\frac{\partial \mathbf{J}}{\partial \mathbf{b}^{(d)}}\mathbf{W}^{(d)} $$

<center>(단, $\mathbf{g}^{(D+1)}=\frac{\partial \mathbf{J}}{\partial \mathbf{y}}$)</center>

그럼 뭔가 끝이 좀 찝찝하긴 하지만, 이 쯤에서 글을 마치겠습니다. 글의 오류 지적은 환영합니다! 모르시는 점이 있으시면 댓글 남겨주시면 제가 아는 선에서 답변을 드리겠습니다. 감사합니다.

[^1]: 굵은 글씨로 표시된 것은 벡터입니다.
[^2]: $$\mathbf{J}$$를 제외한 다른 굵은 큰 글씨로 표시된 것은 행렬입니다.